{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset = load_dataset(\"empathetic_dialogues\", trust_remote_code=True)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-26T05:47:19.014481Z","iopub.execute_input":"2025-06-26T05:47:19.014667Z","iopub.status.idle":"2025-06-26T05:47:31.949152Z","shell.execute_reply.started":"2025-06-26T05:47:19.014649Z","shell.execute_reply":"2025-06-26T05:47:31.948391Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a9ca5d189fe41c5b28d7df2a72c06d2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"empathetic_dialogues.py: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb30a1eda177486887bccd2088dc440e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/28.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42ddbbd5319c421dbdeac3cf9b8ca796"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/76673 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c78a1d8b8f34a58bc97fb3dd20b39c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/12030 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4cfc41c7d4fe421a88df6258b152fa3b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/10943 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5e98f8adeee4efebafcc0fde803b279"}},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"train_dataset = dataset[\"train\"]\nval_dataset = dataset[\"validation\"]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T05:47:31.949961Z","iopub.execute_input":"2025-06-26T05:47:31.950274Z","iopub.status.idle":"2025-06-26T05:47:31.953500Z","shell.execute_reply.started":"2025-06-26T05:47:31.950256Z","shell.execute_reply":"2025-06-26T05:47:31.952871Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\ntokenizer.pad_token = tokenizer.eos_token\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T05:47:31.954988Z","iopub.execute_input":"2025-06-26T05:47:31.955192Z","iopub.status.idle":"2025-06-26T05:47:46.329739Z","shell.execute_reply.started":"2025-06-26T05:47:31.955177Z","shell.execute_reply":"2025-06-26T05:47:46.329114Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1a4129e96db40bf98a6c8adbd32c471"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07dfce9c0bb34aef843208798eac1ed6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6190bbb7c0fd4474bf82b0b135a8844c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db6e638fc7db49aeb7342fdd2ffe3a3a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a106db6d93149b48f507257907d53df"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"from datasets import Dataset, DatasetDict\nfrom tqdm import tqdm\nfrom collections import defaultdict\n\n\ndef build_context_response_pairs(dataset):\n    input_texts = []\n    target_texts = []\n\n    # Group utterances by conversation ID\n    conversations = defaultdict(list)\n    for example in dataset:\n        conversations[example[\"conv_id\"]].append(example)\n\n    for conv in conversations.values():\n        if len(conv) < 2:\n            continue  # Skip conversations that are too short\n\n        # Sort by utterance index to preserve order\n        conv = sorted(conv, key=lambda x: x[\"utterance_idx\"])\n\n        # Determine speaker roles based on the first switch\n        first_speaker = conv[0][\"speaker_idx\"]\n        bot_speaker = None\n\n        for utt in conv:\n            if utt[\"speaker_idx\"] != first_speaker:\n                bot_speaker = utt[\"speaker_idx\"]\n                break\n\n        if bot_speaker is None:\n            continue  # Conversation never switches speakers\n\n        user_speaker = first_speaker\n\n        # Get the prompt only once\n        prompt = conv[0][\"prompt\"]\n        history = f\"Prompt: {prompt}\\nHistory: \"\n        Utterance = f\"Utterance: \"\n\n        for i in range(len(conv)):\n            speaker = conv[i][\"speaker_idx\"]\n            utterance = conv[i][\"utterance\"]\n            emotion = conv[i][\"context\"]\n\n            # Assign role\n            role = \"Bot\" if speaker == bot_speaker else \"User\"\n            if role != \"Bot\":\n                \n                Utterance += f\"{role}: {utterance}\\n\"\n\n            # If it's a bot turn and there is at least one user input before\n            if role == \"Bot\" and i > 0:\n                context_text = history.strip()\n                input_texts.append(f\"{context_text}\\n{Utterance.strip()}\")\n\n                target_texts.append(f\"{emotion}\\n{utterance}\")\n            if role == \"Bot\":\n                Utterance = f\"Utterance: \"\n            history += f\"{role}: {utterance}\\n\"\n            \n\n    return Dataset.from_dict({\"input\": input_texts, \"output\": target_texts})\n\ndef tokenize(example):\n    input_text = f\"Context:\\n{example['input']}\\nResponse:\\n\"\n    target_text = example[\"output\"]\n\n    # Combine input and target\n    full_text = input_text + target_text\n\n    # Tokenize the full sequence\n    tokens = tokenizer(\n        full_text,\n        padding=\"max_length\",\n        truncation=True,\n        max_length=256,\n        return_tensors=None\n    )\n\n    # Calculate how many tokens belong to input (to mask them from loss)\n    input_ids_only = tokenizer(\n        input_text,\n        truncation=True,\n        max_length=256,\n        return_tensors=None\n    )[\"input_ids\"]\n\n    input_len = len(input_ids_only)\n\n    # Create label mask: -100 for input tokens, actual tokens for target\n    labels = [-100] * input_len + tokens[\"input_ids\"][input_len:]\n    labels = labels[:256]  # Ensure same length as input_ids\n    tokens[\"labels\"] = labels\n\n    return tokens\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T05:51:37.370297Z","iopub.execute_input":"2025-06-26T05:51:37.370978Z","iopub.status.idle":"2025-06-26T05:51:37.379877Z","shell.execute_reply.started":"2025-06-26T05:51:37.370957Z","shell.execute_reply":"2025-06-26T05:51:37.379194Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"train_formatted = build_context_response_pairs(train_dataset)\nval_formatted = build_context_response_pairs(val_dataset)\n\ntokenized_train = train_formatted.map(tokenize)\ntokenized_val = val_formatted.map(tokenize)\n\ntokenized_train.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\ntokenized_val.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T05:51:41.180651Z","iopub.execute_input":"2025-06-26T05:51:41.181233Z","iopub.status.idle":"2025-06-26T05:52:25.188962Z","shell.execute_reply.started":"2025-06-26T05:51:41.181210Z","shell.execute_reply":"2025-06-26T05:52:25.188251Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/36629 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d463228062e147fb9585148371686145"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5712 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6716303db032482d94b5f09ff5c0dfcf"}},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"for n in range(0, 8):\n    print(train_formatted[n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T05:52:32.831640Z","iopub.execute_input":"2025-06-26T05:52:32.832180Z","iopub.status.idle":"2025-06-26T05:52:32.837927Z","shell.execute_reply.started":"2025-06-26T05:52:32.832158Z","shell.execute_reply":"2025-06-26T05:52:32.837290Z"}},"outputs":[{"name":"stdout","text":"{'input': 'Prompt: I remember going to the fireworks with my best friend. There was a lot of people_comma_ but it only felt like us in the world.\\nHistory: User: I remember going to see the fireworks with my best friend. It was the first time we ever spent time alone together. Although there was a lot of people_comma_ we felt like the only people in the world.\\nUtterance: User: I remember going to see the fireworks with my best friend. It was the first time we ever spent time alone together. Although there was a lot of people_comma_ we felt like the only people in the world.', 'output': 'sentimental\\nWas this a friend you were in love with_comma_ or just a best friend?'}\n{'input': 'Prompt: I remember going to the fireworks with my best friend. There was a lot of people_comma_ but it only felt like us in the world.\\nHistory: User: I remember going to see the fireworks with my best friend. It was the first time we ever spent time alone together. Although there was a lot of people_comma_ we felt like the only people in the world.\\nBot: Was this a friend you were in love with_comma_ or just a best friend?\\nUser: This was a best friend. I miss her.\\nUtterance: User: This was a best friend. I miss her.', 'output': 'sentimental\\nWhere has she gone?'}\n{'input': 'Prompt: I remember going to the fireworks with my best friend. There was a lot of people_comma_ but it only felt like us in the world.\\nHistory: User: I remember going to see the fireworks with my best friend. It was the first time we ever spent time alone together. Although there was a lot of people_comma_ we felt like the only people in the world.\\nBot: Was this a friend you were in love with_comma_ or just a best friend?\\nUser: This was a best friend. I miss her.\\nBot: Where has she gone?\\nUser: We no longer talk.\\nUtterance: User: We no longer talk.', 'output': 'sentimental\\nOh was this something that happened because of an argument?'}\n{'input': 'Prompt:  i used to scare for darkness\\nHistory: User:  it feels like hitting to blank wall when i see the darkness\\nUtterance: User:  it feels like hitting to blank wall when i see the darkness', 'output': \"afraid\\nOh ya? I don't really see how\"}\n{'input': \"Prompt:  i used to scare for darkness\\nHistory: User:  it feels like hitting to blank wall when i see the darkness\\nBot: Oh ya? I don't really see how\\nUser: dont you feel so.. its a wonder\\nUtterance: User: dont you feel so.. its a wonder\", 'output': 'afraid\\nI do actually hit blank walls a lot of times but i get by'}\n{'input': \"Prompt:  i used to scare for darkness\\nHistory: User:  it feels like hitting to blank wall when i see the darkness\\nBot: Oh ya? I don't really see how\\nUser: dont you feel so.. its a wonder \\nBot: I do actually hit blank walls a lot of times but i get by\\nUser:  i virtually thought so.. and i used to get sweatings\\nUtterance: User:  i virtually thought so.. and i used to get sweatings\", 'output': 'afraid\\nWait what are sweatings'}\n{'input': 'Prompt: I showed a guy how to run a good bead in welding class and he caught on quick.\\nHistory: User: Hi how are you doing today\\nUtterance: User: Hi how are you doing today', 'output': 'proud\\ndoing good.. how about you'}\n{'input': 'Prompt: I showed a guy how to run a good bead in welding class and he caught on quick.\\nHistory: User: Hi how are you doing today\\nBot: doing good.. how about you\\nUser: Im good_comma_ trying to understand how someone can feel like hitting a blank wall when they see the darkness\\nUtterance: User: Im good_comma_ trying to understand how someone can feel like hitting a blank wall when they see the darkness', 'output': \"proud\\nit's quite strange that you didnt imagine it\"}\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM\nimport torch\n\nmodel = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\nmodel.resize_token_embeddings(len(tokenizer))  # Match tokenizer\nmodel.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T05:54:20.228776Z","iopub.execute_input":"2025-06-26T05:54:20.229497Z","iopub.status.idle":"2025-06-26T05:54:37.488011Z","shell.execute_reply.started":"2025-06-26T05:54:20.229475Z","shell.execute_reply":"2025-06-26T05:54:37.487444Z"}},"outputs":[{"name":"stderr","text":"2025-06-26 05:54:23.013299: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1750917263.274833      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1750917263.353385      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"775672a13fa74f8389b545dd21ffe61b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5d674538f3f41ec847d585fe8b73886"}},"metadata":{}},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"GPT2LMHeadModel(\n  (transformer): GPT2Model(\n    (wte): Embedding(50257, 768)\n    (wpe): Embedding(1024, 768)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-5): 6 x GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D(nf=2304, nx=768)\n          (c_proj): Conv1D(nf=768, nx=768)\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D(nf=3072, nx=768)\n          (c_proj): Conv1D(nf=768, nx=3072)\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n)"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"from transformers import TrainingArguments, EarlyStoppingCallback, Trainer\n\ntraining_args = TrainingArguments(\n    output_dir=\"./empathetic_bot_v2\",\n    num_train_epochs=4,                         # ðŸ‘ˆ With large datasets, 1â€“2 epochs may be enough\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    gradient_accumulation_steps=2,              # ðŸ‘ˆ Simulates larger effective batch size (32)\n    \n    eval_strategy=\"steps\",\n    eval_steps=1000,                            # ðŸ‘ˆ Evaluate less frequently for large data\n    save_strategy=\"steps\",\n    save_steps=1000,\n    logging_steps=200,\n\n    fp16=True,\n    save_total_limit=2,\n    load_best_model_at_end=True,\n\n    weight_decay=0.01,                          # ðŸ‘ˆ Regularization (L2)\n    learning_rate=3e-5,                         # ðŸ‘ˆ Conservative LR for stability\n    lr_scheduler_type=\"cosine_with_restarts\",   # ðŸ‘ˆ Often generalizes better\n    warmup_ratio=0.1,                           # ðŸ‘ˆ Auto-scaling warmup\n\n    report_to=\"none\"\n)\n\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train,\n    eval_dataset=tokenized_val,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T05:54:52.495548Z","iopub.execute_input":"2025-06-26T05:54:52.496116Z","iopub.status.idle":"2025-06-26T05:54:54.236530Z","shell.execute_reply.started":"2025-06-26T05:54:52.496094Z","shell.execute_reply":"2025-06-26T05:54:54.235526Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"trainer.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T05:55:06.534788Z","iopub.execute_input":"2025-06-26T05:55:06.535496Z","iopub.status.idle":"2025-06-26T06:53:37.328387Z","shell.execute_reply.started":"2025-06-26T05:55:06.535473Z","shell.execute_reply":"2025-06-26T06:53:37.327811Z"}},"outputs":[{"name":"stderr","text":"`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4580' max='4580' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4580/4580 58:28, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1000</td>\n      <td>0.308600</td>\n      <td>0.333320</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.293500</td>\n      <td>0.325321</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.285300</td>\n      <td>0.322288</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.276600</td>\n      <td>0.321297</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=4580, training_loss=0.4123884992307971, metrics={'train_runtime': 3510.3756, 'train_samples_per_second': 41.738, 'train_steps_per_second': 1.305, 'total_flos': 9571038677434368.0, 'train_loss': 0.4123884992307971, 'epoch': 4.0})"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"model.save_pretrained(\"empathetic-kaggle_v2\")\ntokenizer.save_pretrained(\"empathetic-kaggle_v2\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T07:12:31.441733Z","iopub.execute_input":"2025-06-26T07:12:31.442058Z","iopub.status.idle":"2025-06-26T07:12:32.254450Z","shell.execute_reply.started":"2025-06-26T07:12:31.442036Z","shell.execute_reply":"2025-06-26T07:12:32.253869Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"('empathetic-kaggle_v2/tokenizer_config.json',\n 'empathetic-kaggle_v2/special_tokens_map.json',\n 'empathetic-kaggle_v2/vocab.json',\n 'empathetic-kaggle_v2/merges.txt',\n 'empathetic-kaggle_v2/added_tokens.json',\n 'empathetic-kaggle_v2/tokenizer.json')"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\n# Load the fine-tuned model from Kaggle path\nmodel_path = \"empathetic-kaggle_v2\"  # Replace with correct path if different\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForCausalLM.from_pretrained(model_path)\nmodel.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef chat():\n    print(\"ðŸ¤– Empathetic Bot is ready. Type your message (type 'quit' to exit)\\n\")\n    history = \"\"\n    while True:\n        user_input = input(\"You: \")\n        if user_input.lower() == \"quit\":\n            break\n        if user_input.lower() == \"new\":\n            history = \"\"\n            continue\n\n        # Format the chat history properly\n        history += f\"User: {user_input}\\nBot:\"\n\n        inputs = tokenizer(history, return_tensors=\"pt\", truncation=True, max_length=512).to(model.device)\n\n        with torch.no_grad():\n            outputs = model.generate(\n                **inputs,\n                max_new_tokens=100,\n                pad_token_id=tokenizer.eos_token_id,\n                temperature=0.7,\n                top_k=50,\n                top_p=0.9,\n                do_sample=True\n            )\n\n        full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n        # Extract only the new response (after last \"Bot:\")\n        response = full_output.split(\"Bot:\")[-1].strip().split(\"User:\")[0].strip()\n        print(f\"Bot: {response}\\n\")\n\n        # Add bot reply to history\n        history += f\" {response}\\n\"\n\n# Start the chatbot\nchat()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T07:28:56.470117Z","iopub.execute_input":"2025-06-26T07:28:56.470392Z","iopub.status.idle":"2025-06-26T07:29:20.104546Z","shell.execute_reply.started":"2025-06-26T07:28:56.470373Z","shell.execute_reply":"2025-06-26T07:29:20.103730Z"}},"outputs":[{"name":"stdout","text":"ðŸ¤– Empathetic Bot is ready. Type your message (type 'quit' to exit)\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  I am sad\n"},{"name":"stdout","text":"Bot: That's sad. I am sorry to hear that.\n\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/2860084031.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;31m# Start the chatbot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mchat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_35/2860084031.py\u001b[0m in \u001b[0;36mchat\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"quit\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"],"ename":"KeyboardInterrupt","evalue":"Interrupted by user","output_type":"error"}],"execution_count":19},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\n# Load the fine-tuned model from Kaggle path\nmodel_path = \"empathetic-kaggle_v2\"  # Replace with correct path if different\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForCausalLM.from_pretrained(model_path)\nmodel.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef chat():\n    print(\"ðŸ¤– Empathetic Bot is ready. Type your message (type 'quit' to exit)\\n\")\n    history = \"\"\n    MAX_HISTORY_LINES = 6\n    \n    while True:\n        user_input = input(\"You: \").strip()\n        if user_input.lower() in [\"quit\", \"exit\"]:\n            break\n        if user_input.lower() in [\"new\", \"reset\"]:\n            history = \"\"\n            print(\"ðŸ” Context reset.\\n\")\n            continue\n    \n        # Add latest user input\n        history += f\"User: {user_input}\\nBot:\"\n    \n        # Trim history\n        history_lines = history.strip().split(\"\\n\")\n        if len(history_lines) > MAX_HISTORY_LINES:\n            history = \"\\n\".join(history_lines[-MAX_HISTORY_LINES:])\n    \n        inputs = tokenizer(history, return_tensors=\"pt\", truncation=True, max_length=512).to(model.device)\n    \n        with torch.no_grad():\n            output_ids = model.generate(\n                **inputs,\n                max_new_tokens=100,\n                pad_token_id=tokenizer.eos_token_id,\n                temperature=0.6,\n                top_k=50,\n                top_p=0.85,\n                do_sample=False,\n                repetition_penalty=1.1,\n                no_repeat_ngram_size=3\n            )\n    \n        full_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n        response = full_output.split(\"Bot:\")[-1].strip().split(\"User:\")[0].strip()\n    \n        print(f\"Bot: {response}\\n\")\n        history += f\" {response}\\n\"\n\n\n# Start the chatbot\nchat()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T07:38:21.822143Z","iopub.execute_input":"2025-06-26T07:38:21.822412Z","iopub.status.idle":"2025-06-26T07:41:26.582991Z","shell.execute_reply.started":"2025-06-26T07:38:21.822394Z","shell.execute_reply":"2025-06-26T07:41:26.582125Z"}},"outputs":[{"name":"stdout","text":"ðŸ¤– Empathetic Bot is ready. Type your message (type 'quit' to exit)\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  I am sad\n"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.85` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Bot: That is terrible.\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  My friend is angry with me.\n"},{"name":"stdout","text":"Bot: Well_comma_ it's understandable that you feel the same way about others\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  But I am not angry with him\n"},{"name":"stdout","text":"Bot: It can be frustrating to have people like yourself who are just trying hard for themselves\n\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/1287638852.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;31m# Start the chatbot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0mchat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_35/1287638852.py\u001b[0m in \u001b[0;36mchat\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"quit\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"exit\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"],"ename":"KeyboardInterrupt","evalue":"Interrupted by user","output_type":"error"}],"execution_count":22},{"cell_type":"code","source":"import shutil\n\n# Compress the folder into a zip file\nshutil.make_archive(\"empathetic-kaggle_v2\", 'zip', \"empathetic-kaggle_v2\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T07:41:52.701632Z","iopub.execute_input":"2025-06-26T07:41:52.701907Z","iopub.status.idle":"2025-06-26T07:42:08.735948Z","shell.execute_reply.started":"2025-06-26T07:41:52.701887Z","shell.execute_reply":"2025-06-26T07:42:08.735342Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/empathetic-kaggle_v2.zip'"},"metadata":{}}],"execution_count":23}]}
